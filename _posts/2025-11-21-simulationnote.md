---
layout:     post
title:      "蒙特卡洛理论及应用"
subtitle:   " \"计算机模拟课程笔记\""
date:       2025-11-21 16:00:00
author:     "Summer"
header-img: "img/head/image6.jpg"
catalog: true
mathjax: true
tags:
    - Mathematical Modeling
---

这是2025-2026秋冬计算机模拟的课程笔记. 因为秋学期实在太摆烂了, 冬学期要好好读一下PPT, 督促自己不要上课摸鱼了~

> 正在施工


## 8 马尔科夫链蒙特卡罗(MCMC)

### 8.1 简单蒙特卡洛(MC)方法的局限性

> **抽样效率**: 指 “用较少的样本量就能得到可靠结果” 的能力。效率越高，越能以低成本（时间 / 计算资源）完成估计；效率低则需要极多样本，甚至无法完成计算。

> **提议分布设计**: 在重要性抽样、拒绝 - 接受抽样中，需要先构造一个容易抽样的分布 q(x) 即 “提议分布”，再基于 q(x) 生成样本并修正，以逼近目标分布 p(x) 。

简单 MC 的局限集中在抽样效率和提议分布上：
- 直接抽样效率低：面对高维问题或稀有事件时，有效样本占比太少，需要极多样本才能得到可靠结果；
- 提议分布相关缺陷：
若提议分布 q(x) 与真实分布 p(x) 差异大，拒绝 - 接受 / 重要性抽样的效率会很低；构造 “匹配 p(x) 的好 q(x)” 本身很困难（因为实际中 p(x) 往往复杂未知）

<img src="https://zzh123-1325455460.cos.ap-nanjing.myqcloud.com/202511221521297.png" width="600"/>

### 8.2 随机过程

**随机过程是随机变量的集合**，记为$\{X_t, t \in T\}$。

其中：
- $T$是“指标集”，可以是离散的（比如$T=\{1,2,3,...\}$，对应“随机变量序列”），也可以是连续的（比如$T=[0,+\infty)$）；
- 每个$X_t$是一个随机变量，整个集合描述了“随指标（通常是时间）变化的随机现象”。


常见的随机过程包括Poisson过程、Markov过程等。

#### 8.2.1 Poisson过程

**Poisson过程是描述“事件在时间轴上随机发生次数”的随机过程**. 

假设事件在$[0, t]$上任意时刻发生, $N(t)$表示$[0,t]$内事件发生的次数. 核心是满足3个条件的计数过程$N(t)$：

1. **初始条件**：$N(0)=0$（初始时刻事件发生次数为0）；
2. **独立增量**：任意不重叠的时间区间内，事件发生的次数相互独立（比如$[0,t_1]$和$[t_1,t_2]$内的发生次数无关）；
3. **增量服从Poisson分布**：对任意时间间隔$t>0$，任意起始时刻$s≥0$，区间$[s,s+t]$内事件发生$k$次的概率满足：
   $$
   P\{N(s+t)-N(s)=k\} = \frac{(\lambda t)^k e^{-\lambda t}}{k!}
   $$
   其中$\lambda>0$是“速率”（表示单位时间内事件发生的平均次数）。



<img src="https://zzh123-1325455460.cos.ap-nanjing.myqcloud.com/202511221529118.png" width="400"/>


简单说，Poisson过程是 **用来刻画随机、独立发生的事件的计数规律** 的模型（比如某路口单位时间内的车流量、客服中心的呼叫次数等）。

#### 8.2.2 Markov过程

**Markov过程是满足“无后效性”的随机过程**，核心特点是“未来的状态只依赖于当前状态，与过去的状态无关”。


对于随机过程$ X = \{X_t, t \in R\} $，若满足：
$$
P(X_{t+1} = x_{t+1} | X_0 = x_0, \dots, X_t = x_t) = P(X_{t+1} = x_{t+1} | X_t = x_t)
$$
简单说：**已知“现在”，“未来”的概率分布与“过去”无关**，这一性质叫“Markov性”（无后效性）。


- 状态可以是离散的（比如“晴、云、雨”），也可以是连续的；
- 指标集（通常是时间）可以是离散的（比如“每天的状态”）或连续的。

举个例子, 比如：
- 青蛙在荷花池的跳跃（下一个位置只和当前位置有关）；
- 天气变化（明天的天气只依赖今天的天气，和昨天无关）；
- 传染病受感染的人数
- 原子核中一自由电子的跳跃
- 人口增长过程
- 股价波动、布朗运动等。

<img src="https://zzh123-1325455460.cos.ap-nanjing.myqcloud.com/202511221533168.png" width="500"/>

#### 8.2.3 Markov Chain 马尔科夫链

**马尔可夫链（Markov Chain）是“离散时间下满足马尔可夫性的随机过程”**. 是按时间顺序排列的一系列状态序列：$ x^{(0)}, x^{(1)}, x^{(2)}, \dots, x^{(n)}, x^{(n+1)}, \dots $，描述状态之间“前后相继”的转换过程。

**连续时间Markov过程**
考虑随机过程 $ X = \{X_t, t \in R\} $，满足：
$$
P(X_{t+1} = x_{t+1} \mid X_0 = x_0, \dots, X_t = x_t) = P(X_{t+1} = x_{t+1} \mid X_t = x_t)
$$

**离散时间马尔可夫链**
- 描述了从状态 $ x_i $ 到状态 $ x_j $ 的转换的随机过程。
- 假设存在一个独立于时间的**固定概率** $ p_{ij} $，使得：
  $$
  P(x^{(n+1)} = i \mid x^{(n)} = j, x^{(n-1)} = i_{n-1}, \dots, x^{(0)} = i_0) = p_{ij}, n \geq 0.
  $$



**马尔科夫性**(无记忆性): 下一个状态的概率**只由当前状态决定**，与过去的所有状态无关：
$$
P(x^{(n+1)} = i \mid x^{(n)} = j, x^{(n-1)} = i_{n-1}, \dots, x^{(0)} = i_0) = p_{ij}
$$
其中 $ p_{ij} $ 是**固定的状态转移概率**（与时间无关）。

#### 8.2.4 转移概率矩阵

考虑一个特殊的情况, 在**齐次马尔可夫链**（状态转移概率与时间无关）中，定义概率 $ p_{ij} $ 为：

$$
p_{ij} = P(X_{t+1} = j \mid X_t = i)
$$

它表示“当前处于状态 $ i $ 时，下一个时刻转移到状态 $ j $ 的概率”。

- 非负性：$ p_{ij} \geq 0 $（概率不能为负）；
- 归一性：$ \sum_{j=0}^{\infty} p_{ij} = 1 $（从状态 $ i $ 转移到所有可能状态的概率之和为1）。

将所有状态的转移概率 $ p_{ij} $ 按“行（当前状态）、列（下一状态）”排列，组成的矩阵：
$$
P = \begin{pmatrix}
p_{00} & p_{01} & \dots \\
p_{10} & p_{11} & \dots \\
\vdots & \vdots & \vdots
\end{pmatrix}
$$
这个矩阵就叫 **（一步）转移概率矩阵** ，它完整描述了马尔可夫链中“状态之间的转移规律”——是转移概率的“模型化表达”。

<img src="https://zzh123-1325455460.cos.ap-nanjing.myqcloud.com/202511221716708.png" width="400"/>

简单说：**转移概率矩阵（转移概率模型）是用矩阵形式，统一记录马尔可夫链中所有状态间的转移概率，从而量化状态转移的规律**。

#### 8.2.5 平稳分布

**平稳分布是马尔可夫链在长期运行后状态概率不再变化的稳定分布**

对于马尔可夫链，若存在一个状态概率分布 $ \pi $，满足：
$$
\pi = \pi \cdot P
$$
（$ P $ 是转移概率矩阵），则 $ \pi $ 称为该马尔可夫链的**平稳分布**。

直观来说：当马尔可夫链的状态概率分布达到 $ \pi $ 后，再经过一次转移，概率分布依然是 $ \pi $（状态概率不再随时间变化）。

<span style="font-size: 20px; color: #C25759;">案例1</span>

我们来看一个案例. **市场状态（牛、熊、平）构成的马尔可夫链，如何收敛到平稳分布**

<img src="https://zzh123-1325455460.cos.ap-nanjing.myqcloud.com/202511221728238.png" width="400"/>

- **状态**：3种市场状态——牛、熊、平；
- **转移矩阵 $ P $**：描述状态间的转移概率（行是当前状态，列是下一状态）：
  $$
  P = \begin{pmatrix}
  0.9 & 0.075 & 0.025 \quad \text{（当前是“牛”，转牛/熊/平的概率）} \\
  0.15 & 0.8 & 0.05 \quad \text{（当前是“熊”，转牛/熊/平的概率）} \\
  0.25 & 0.25 & 0.5 \quad \text{（当前是“平”，转牛/熊/平的概率）}
  \end{pmatrix}
  $$
- **初始状态分布 $ s $**：初始时处于“牛、熊、平”的概率是 $ [0.12, 0.23, 0.65] $。

<img src="https://zzh123-1325455460.cos.ap-nanjing.myqcloud.com/202511221730197.png" width="300"/>

通过循环让初始分布 $ s $ 不断乘以转移矩阵 $ P $（模拟状态随时间的转移），可以看到：
- 初始几步（比如Step2），分布变化较大；
- 随着迭代次数增加（到Step40左右），分布逐渐稳定在 $ [0.62500, 0.31250, 0.06250] $；
- 之后无论再迭代多少次，分布都不再变化。


最终稳定的 $ [0.625, 0.3125, 0.0625] $ 就是这个马尔可夫链的**平稳分布**. 意味着“长期来看，市场处于‘牛’的概率是62.5%，‘熊’是31.25%，‘平’是6.25%”，且这个概率会保持稳定, 不再随时间变化。

<span style="font-size: 20px; color: #C25759;">案例2 : 周期性转移</span>


- **状态**：3个状态（1、2、3）；
- **转移概率矩阵 $ P $**：
  $$
  P = \begin{pmatrix}
  0 & 1 & 0 \quad \text{（状态1只能转移到状态2）} \\
  0.5 & 0 & 0.5 \quad \text{（状态2以0.5概率转移到1或3）} \\
  0 & 1 & 0 \quad \text{（状态3只能转移到状态2）}
  \end{pmatrix}
  $$
- **转移图规律**：状态1→2→1/3→2→1/3…，呈现“2步一循环”的周期特征。

<img src="https://zzh123-1325455460.cos.ap-nanjing.myqcloud.com/202511221758476.png"/>

通过计算转移矩阵的幂可以验证：$ P^3 = P $（即3次转移等价于1次转移），说明该马尔可夫链的**周期为2**——从任意状态出发，回到原状态的步数必须是2的倍数（比如状态1→2→1，需要2步；状态3→2→3，也需要2步）。

尽管是周期链，它依然存在**不变分布（平稳分布）**：$ \pi = (0.3636, 0.2397, 0.3967) $。
但注意：周期链的平稳分布是“长期平均的状态占比”，但状态不会固定在某一分布，而是按周期波动（但长期均值稳定）。



MCMC抽样（马尔可夫链蒙特卡洛，Markov Chain Monte Carlo）是一种非常强大的统计方法，它的核心思想是：

**当你无法直接从一个非常复杂的目标（比如一个形状极其古怪的山）上随机取样（比如随机空投抓人）时，MCMC提供了一种“间接”的取样方法。**

它通过**构造一个“智能”的随机游走过程（马尔可夫链）**，让这个游走过程最终在山上“待得久”的地方，恰好就是山峰（概率高）的地方。这样，你只需要记录下这个游走者的足迹，就相当于得到了你想要的样本。

为了理解这个过程，我们把它拆分成两个部分来看：

### 1. 蒙特卡洛（Monte Carlo）

* **这是什么？** 这是一种思想，指通过大量的**随机抽样**来估算一个（通常很难直接计算的）结果。
* **举个例子：** 你想知道一个不规则湖泊的面积。你无法用尺子量。怎么办？
    1.  你在这个湖泊周围画一个巨大的、面积已知的正方形（比如1000平方米）。
    2.  你朝着这个正方形**随机**扔下10000块石头（这就是抽样）。
    3.  你数一数，发现有3000块石头掉进了湖里。
    4.  你就可以**估算**出湖的面积大约是正方形面积的 3000/10000 = 30%，即1000 * 0.3 = 300平方米。
* **问题：** 蒙特卡洛方法的前提是你能做到“真正的随机抽样”（比如在正方形内均匀地扔石头）。但如果目标分布（比如那个湖的形状）非常复杂、维度非常高，你根本没法直接进行均匀抽样。这时就需要马尔可夫链登场了。

### 2. 马尔可夫链（Markov Chain）

* **这是什么？** 这是一个随机过程，它有一个关键特性，叫做**“无记忆性”**。
* **无记忆性：** 过程中的**下一个状态只取决于当前状态**，而与“如何”到达当前状态的“历史路径”完全无关。
* **举个例子：** 想象一个“健忘的”天气预报。
    * 如果**今天**是晴天，那么**明天**有80%的概率是晴天，20%的概率是雨天。
    * 如果**今天**是雨天，那么**明天**有60%的概率是晴天，40%的概率是雨天。
    * 明天是晴是雨，**只**跟今天是晴是雨有关，跟昨天、前天是晴是雨**无关**。

* **关键特性（平稳分布）：** 只要这个“马尔可夫链”设计得当（满足某些条件，比如你PPT上的“不可约性”和“对称性”），无论它从什么状态开始（第一天是晴是雨都无所谓），在它随机游走了足够长的时间后，它处于各个状态的**概率会趋于一个稳定的分布**。
    * 比如，在上面的例子中，长期来看，这个地方可能75%的日子是晴天，25%的日子是雨天。这个（75%, 25%）就是它的**平稳分布**（Stationary Distribution）。

---

### MCMC（两者结合）：如何抽样？

MCMC的绝妙之处在于**反向利用**了马尔可夫链的特性：

> 我们的**目标**是想从一个特定的、复杂的概率分布 $P(x)$ 中抽样（比如这个分布就是我们想找到的“平稳分布”）。
>
> 我们的**方法**是设计一个马尔可夫链（即设计一套“游走”的规则），使得这个马尔可夫链的平稳分布**恰好**就是我们想要的那个 $P(x)$。

**这个“游走”过程大致如下（以Metropolis-Hastings算法为例）：**

1.  **开始：** 随机在“山上”（目标分布）选一个点 $x_t$ 作为**当前位置**。
2.  **提议：** 在当前位置 $x_t$ 附近，随机“提议”一个新的位置 $x'$。（比如，随机向左或向右走一步）。
3.  **决策（接受-拒绝）：**
    * **如果新位置 $x'$ 更高（概率更大）：** 那么“好，这个提议不错”，**接受**这个新位置。游走者移动到 $x'$，把 $x'$ 作为下一个样本。
    * **如果新位置 $x'$ 更低（概率更小）：** **不立即拒绝！** 这就是MCMC的关键。我们以**一定的概率**接受这个“更差”的移动（这个概率与新旧位置的高度差有关）。如果接受，就移动到 $x'$；如果拒绝，就**停在原地** $x_t$ 不动，并把 $x_t$ **再次**记录为一个样本。
4.  **重复：** 不断重复步骤2和3。

**为什么这么做？**

* **模拟退火（你PPT上的内容）就是MCMC的一种。** “接受更差解”的概率就是那个 $e^{-\frac{\Delta f}{T}}$。
* 这种“有概率接受更差解”的机制（就像模拟退火中的高温阶段），保证了游走者**不会被困在某个局部的山峰（局部最优解）**，而是能跑遍整个山脉，去探索全局最高峰（全局最优解）。
* 通过这个巧妙的“接受-拒绝”规则，我们保证了游走者在“高处”（概率高的地方）停留的时间更长，在“低处”（概率低的地方）停留的时间更短。
* **Burn-in（预热期）：** 刚开始时，游走者可能还在“山脚”或一个很偏僻的位置。它需要一段时间才能“走到”主要的山脉区域。因此，我们通常会丢弃最开始的一段足迹（比如前1000步），这叫作**Burn-in**。
* **抽样：** 在Burn-in之后，我们开始收集游走者的足迹（例如，每隔10步记录一次位置）。这些被收集到的位置点，**就是从我们想要的目标分布 $P(x)$ 中抽出的样本**。

**总结：**

**MCMC抽样**就是通过设计一个“智能的随机游走”（马尔可夫链），让它在目标概率分布图上“闲逛”。由于规则的设计，它在概率高的地方逛得久，在概率低的地方逛得少。我们通过收集它“闲逛”时的足迹，就等价于从这个复杂的分布中抽取了样本。

## 9 模拟退火 -- 统计物理




## 10 仿真优化算法 -- 非线性优化

### 10.1 遗传算法

一种模拟生物在自然环境中的遗传和进化的过程而形成的自适应全局优化概率搜索算法

五个核心要素：参数编码、种群初始化、遗传算子设计、适应度函数设计、控制参数

<span style="font-size: 20px;color: #4F845C;">求`shekel`函数最小值</span>

Shekel函数在这里是一个**单变量测试函数**，常用于优化算法（比如遗传算法）的性能测试，它的用法可以从这几个方面理解：

先把`shekel`函数保存为`.m`文件（比如`shekel.m`），代码功能是**计算输入`x`对应的函数值**：

```matlab
function y = shekel(x)
    % 输入：x（单个数值或数组）
    % 输出：y（对应x的shekel函数值）
    y = 1./(10.295*(x-7.382).^2 + 0.1613) + ...
        1./(4.722*(x-1.972).^2 + 0.2327) + ...
        1./(10.928*(x-8.111).^2 + 0.2047);
end
```

这个Shekel函数是**求最小值的目标函数**（因为优化任务是找它在`2≤x≤9`的最小值）。

在全局优化中（比如用Matlab的遗传算法`ga`），它的角色是：
- 算法会不断尝试不同的`x`（在`2~9`区间内），调用`shekel(x)`计算函数值；
- 最终找到使`shekel(x)`最小的`x`（即“最小点”）和对应的函数值（即“最小值”）。

用Matlab全局优化工具箱的`ga`（遗传算法）求解时，代码格式是：

```matlab
[x, v] = ga(@shekel, 1, [], [], [], [], 2, 9);
```

- `@shekel`：指定目标函数是`shekel`；
- `1`：变量个数（这里是单变量`x`）；
- 中间的`[]`：遗传算法的约束/参数默认值；
- `2, 9`：变量`x`的上下界（`2≤x≤9`）；
- 输出`x`：使`shekel(x)`最小的自变量值；
- 输出`v`：对应的最小值（即`shekel(x)`）。

#### 10.1.1 参数编码

目的: 将优化问题转换为组合问题

常用编码方案: 二进制编码、整数编码、实数编码

1\. **二进制编码**. 把变量转化为**二进制字符串（0和1组成）**的形式。
- 例子：若变量`x∈[0,10]`，精度到0.1，则需要`log2(10/0.1)=7`位二进制（比如`1010101`对应某个x值）。
- 优势：编码/解码简单、遗传操作（交叉/变异）容易实现、种群多样性好。
- 劣势：编码长度可能很长（精度越高越长）、不能直接体现变量的实际意义。


2\. **整数编码**. 把变量直接用**整数**表示（适用于变量本身是整数的问题）。
- 例子：求解“安排5个任务的顺序”，变量是任务编号`1、2、3、4、5`，直接用整数序列`[3,1,5,2,4]`表示解。
- 优势：适合整数类优化问题（如调度、排列）、编码简洁。
- 劣势：只适用于整数变量，不适合连续型变量。

3\. **实数编码**. 直接用变量的 **实际数值(浮点数)** 作为编码（适用于连续型变量）。
- 例子：变量`x∈[2,9]`，直接用`4.86`（Shekel函数的最小点）作为编码。
- 优势：无需编码/解码、能直接反映变量的实际意义、适合连续优化问题。
- 劣势：遗传操作（如变异）需控制范围（避免偏离解空间）、容易提前收敛（种群多样性不足）。

4\. **格雷码**

核心特点是“相邻整数对应的编码只有1位不同”，可以优化遗传算法的局部搜索能力。

以“相邻数`175`和`176`”为例：
- 普通二进制编码：`0010101111`→`0010110000`（5位不同）；
- 格雷码编码：`0010100100`→`0010101100`（仅1位不同）。

这种特性让遗传算法的**局部搜索更平滑**：微小的编码变化对应微小的变量变化，避免普通二进制“编码变多位、变量跳变”的问题。

- 遗传操作（交叉、变异）和普通二进制一样简单；
- 符合“最小字符集编码”（只用0/1），也适用模式定理分析；
- 解码时需通过“格雷码→二进制→十进制”的步骤（有固定转换公式）。

<img src="https://zzh123-1325455460.cos.ap-nanjing.myqcloud.com/202511210843390.png" width="300"/>

#### 10.1.2 初始化种群


初始化种群是遗传算法**开始阶段生成初始解集合**的步骤，是算法后续进化（交叉、变异等）的基础，核心概念和要点如下：

1\. **核心概念**

遗传算法里用“生物遗传术语”类比优化问题：
- **基因（Gene）**：编码后的最小单元（比如二进制编码里的1位0/1）；
- **染色体（Chromosome）/个体（Individual）**：1个“解”的完整编码（比如A1是一条染色体，由多个基因组成）；
- **种群（Population）**：多个个体（染色体）的集合（比如A1-A4组成一个种群）。

<img src="https://zzh123-1325455460.cos.ap-nanjing.myqcloud.com/202511210851394.png" width="300"/>

2\. **初始化的操作方式**

就是**生成n个初始个体（染色体）**，常用2种策略：
- **策略1（利用先验知识）**：如果知道解的大致分布（比如Shekel函数的最小值在4-5附近），就在这个区域内随机生成个体，能加快后续收敛；
- **策略2（冷启动）**：完全随机生成个体（覆盖整个解空间），适合对解分布无了解的情况，之后通过迭代优化。

3\. **种群规模的影响**
   
种群里个体的数量（n）要合理设置：
- 规模**过小**：种群多样性不足，容易“提前收敛”（没找到全局最优就停了）；
- 规模**过大**：计算量会增加，拖慢算法运行速度。

#### 10.1.3 遗传算子

定义三种随机算子, **选择**, **重组** 与 **变异** .生成状态空间 $ \Omega $ 上的 Markov 链 $X_{t}$ . 可以证明, 通过合适地定义上述三种随机算子，生成的链 {$ X_{t}$} 总是不可约的和非周期的，且收敛于一个稳定分布！

1\. **选择(selection)**

在遗传算法中，“选择”是**从当前种群里挑选优秀个体，让它们进入下一代繁衍**的核心操作，目的是保留适应度高（更接近最优解）的个体，推动种群进化。它的核心逻辑是“以适应度为权重，概率性地筛选个体”，常见策略有3种：

轮盘赌选择策略. 把种群的“适应度总和”比作一个“轮盘”，每个个体的适应度占轮盘的“扇形面积比例”，就是它被选中的概率。例如, 适应度高的个体（如图中“3”）占比38%，被选中的概率远高于适应度低的个体（如“2”仅占5%）。简单直接，但选择误差大, 可能漏掉优秀个体，或选入较差个体。

<img src="https://zzh123-1325455460.cos.ap-nanjing.myqcloud.com/202511210905973.png" width="300"/>

随机竞争选择. 先按轮盘赌选2个个体，再让这两个个体“竞争”——适应度高的那个被选中，重复这个过程直到选满种群规模。类似“淘汰赛”，能减少轮盘赌的随机误差，更倾向选优秀个体。

最佳保留选择. 分两步：
① 先按轮盘赌选大部分个体；
② 把当前种群中**适应度最高的个体直接复制到下一代**（避免优秀个体被随机操作淘汰）。
- 特点：既能保留优秀个体，又兼顾种群多样性，是常用的“保优”策略。

2\. **交叉(crossover)**

**二进制编码**：通常方式是1步交叉，即将父母一方的k位初始序列取代第二个亲本的初始k个位置形成新的数位串，以产生后代，其中位置k被随机选择。

<img src="https://zzh123-1325455460.cos.ap-nanjing.myqcloud.com/202511210908407.png" width="400"/>

**实值编码**：先随机选择2个parent，然后选择部分进行交换：比如群体的size是N，先选择最好的M个，就需要在产生N-M个才能维持好种群的大小。假设每个染色体的长度是K，那么先把M个种群中的染色体的第一个基因，按照倍数复制成长度为N-M，接着随机再给这个N-M排序下，组成了N-M个后代的第一个基因，以此类推，就完成了任意2个parent的基因交换和重组。

<img src="https://zzh123-1325455460.cos.ap-nanjing.myqcloud.com/202511210908859.png" width="400"/>

3\. **变异(mutation)**

二进制编码：随机选择二进制串的一位或多位进行翻转或逆序

实数值编码：遍历N个染色体上的K个基因，然后产生随机数，如果这个随机数小于X，就给这个基因做mutation，如果大于则不更改，具体的实现的时候可以灵活处理。

<img src="https://zzh123-1325455460.cos.ap-nanjing.myqcloud.com/202511210909803.png" width="600"/>


#### 10.1.4 遗传算法的流程

遗传算法的流程是 **生成初始解→迭代优化→输出最优解** 的循环过程，核心是通过“选择、交叉、变异”模拟生物进化，逐步找到更优的解。具体步骤（结合图中逻辑）如下：

1\. 初始化种群
生成一批初始个体（解的编码形式），作为算法的“初始候选解集合”。

2\. 计算适应度
对种群中每个个体，计算其**适应度**. 比如求最小值时，函数值越小则适应度越高。

3\. 选择
以“适应度为权重”中，从当前种群中筛选出部分个体，作为“繁衍下一代的父/母本”。例如, 依照公式 $ p_i = \frac{f_i}{\sum_{i=1}^n f_i} $ 抽取个体 $x_{i}$ , 其中 $f_i$ 是个体的适应度

4\. 遗传（复制、交叉、变异）
对选中的个体，按概率执行3种操作（要求 $ p_r + p_c + p_m = 1 $，即三个操作的概率和为1）：
- **复制**（概率 $ p_r $）：直接把优秀个体复制到下一代；
- **交叉**（概率 $ p_c $）：让两个个体交换部分编码（类似“基因重组”），生成新个体；
- **变异**（概率 $ p_m $）：随机改变个体的某一位编码（类似“基因突变”），增加种群多样性。

5\. 终止条件判断
重复步骤2-4，直到满足终止条件：
- 达到最大迭代次数/运行时间（比如1万代）；
- 种群的适应度不再显著变化（说明已接近最优解）。

6\. 输出结果
终止后，输出种群中适应度最高的个体，作为最终的最优解。

### 10.2 n:m 积和式优化问题

**行列式 ($\det(C)$)**：我们在计算行列式时，对不同排列的乘积项会根据逆序数的奇偶性加上正号或负号（即公式中的 $\operatorname{sign}(\sigma)$）。
$$\det(C) = \sum_{\sigma} \text{sign}(\sigma) \prod_{i=1}^{n} c_{i,\sigma(i)}$$

**积和式 ($\operatorname{perm}(C)$)**：积和式的定义与行列式非常相似，唯一的区别是**没有符号项**。它将所有排列的乘积直接相加。
$$\operatorname{perm}(C) = \sum_{\sigma} \prod_{i=1}^{n} c_{i, \sigma(i)}$$
简单来说，就是从矩阵的每一行取一个元素，确保这些元素不在同一列，将它们乘起来，然后把所有可能的这种组合加在一起。运算量为 $O(n!)$，这是一个非常巨大的计算量，比行列式的计算难得多。

积和式的性质:
- 若𝐶中有一个0元素，则积和式中有(n-1)!项为0
- 若𝐶的某一行或某一列元素为0，则积和式为0
- 交换矩阵的任意两行（列）不改变积和式的值

$n:m$ 积和式优化问题是一个**组合优化问题**。

* **设定**：
    1.  你有一个 $n$ 阶方阵（$n \times n$ 的格子）。
    2.  这是一个 **0-1 矩阵**（格子里的数字只能是 0 或 1）。
    3.  你手里总共有 **$m$ 个 "1"**（即矩阵中元素为 1 的个数总和是 $m$）。
* **目标**：
    你需要把这 $m$ 个 "1" 填入 $n \times n$ 的格子中，使得最后计算出来的**积和式 ($\operatorname{perm}(C)$) 的值最大**。


#### <span style="color: #4F845C;">举个例子, 3:5 积和式问题</span>

下面是一个具体的例子：
* **$n = 3$**：这是一个 3阶矩阵。
* **$m = 5$**：矩阵里总共有 5 个 "1"。
* **结果**：最大积和式是 **2**。

**为什么是 2？让我们看 PPT 中的矩阵例子：**
$$
\begin{pmatrix}
1 & 1 & 0 \\
1 & 1 & 0 \\
0 & 0 & 1
\end{pmatrix}
$$

**如何计算这个矩阵的积和式？**
我们需要找出所有“每行取一个，每列取一个”且元素都为 1 的组合：
1.  取 $c_{11}, c_{22}, c_{33}$ (即 $1 \times 1 \times 1$) = 1
2.  取 $c_{12}, c_{21}, c_{33}$ (即 $1 \times 1 \times 1$) = 1
3.  其他任何组合都会包含至少一个 0（例如 $c_{13}$ 是 0）。

所以，$\operatorname{perm}(C) = 1 + 1 = 2$。

**关于矩阵的不唯一性：**

$$
\text{perm}\begin{pmatrix}
0 & 0 & 1 \\
1 & 1 & 0 \\
1 & 1 & 0
\end{pmatrix}
= \text{perm}\begin{pmatrix}
1 & 0 & 1 \\
0 & 1 & 0 \\
1 & 0 & 1
\end{pmatrix}
= 2.
$$

这里展示了另外两个矩阵，它们的形状不同（行或列交换了），但积和式的结果也是 2。这对应了左下角框中的性质 3：**"交换矩阵的任意两行（列）不改变积和式的值"**。这意味着在优化问题中，我们关心的是 1 的**相对结构**（比如是否形成了紧密的块状），而不是它们的绝对位置。

**$n:m$ 积和式优化问题**就是在问：
<span style="font-size: 20px; color: #C25759;">给定 $n \times n$ 的空间和 $m$ 个资源（1），如何布局这些资源，才能让“全排列匹配”的路径数量最多？</span>

* 如果 $m < n$，积和式一定是 0（因为无法选出 $n$ 个 1）。
* 如果 $m = n^2$（全 1 矩阵），积和式是 $n!$。
* 这个问题的难点在于当 $n < m < n^2$ 时，如何分布这些 1 来最大化结果。

#### 10.2.2 使用遗传算法解积和式优化问题

1\. **定义染色体**. 用 $m$ 个 1 的 $n$ 阶 0-1 矩阵作为染色体。

> **回顾编码原则**
> - **有意义**：应使用能易于与所求问题相关。
> - **最小字符集**。

2\. **定义适应度函数**. 即矩阵的积和式 $\operatorname{perm}()$. 数值越大，说明该个体的“基因”越好，被遗传到下一代的概率就越高。

3\. 定义遗传算子（变异和交叉）. 

**变异**：东南西北环绕式的领域交换. 可以看到中心有一个深色格子（代表 1），周围标有 **E (East), W (West), N (North), S (South)** 的格子。这是一个**局部搜索**策略。不是在矩阵里随机找个地方把 0 变成 1，而是选中一个现有的 1，将其移动到它相邻（上、下、左、右）的空白位置（0）。这种“微调”可以避免破坏矩阵的整体结构，尝试在局部寻找更优解。


<img src="https://zzh123-1325455460.cos.ap-nanjing.myqcloud.com/202511211112551.png" width="500"/>

Matlab实现方法

```matlab
function childmat = mutate(childmat)
[lx,ly] = size(childmat);
x = randi(lx); % 随机生成整数索引。
y = randi(ly);
% “东南西北环绕式的领域交换”
% 构建候选邻居坐标数组
ewns = [x+1, x-1, x, x; y, y, y-1, y+1];  
for i = 1:8 % 超出边界的情况周期化，如左图
    if ewns(i) == 0 % 上面溢出处理, 设为最大值
    ewns(i) = lx;
    elseif ewns(i) == lx+1 % 下面溢出处理, 设为1
    ewns(i) = 1;
    end
end
for i = 1:4 % 交换循环
    coord = ewns(i,:);
    if childmat(coord(1), coord(2)) ~= childmat(x,y)
        childmat(coord(1), coord(2)) = …
        1 - childmat(coord(1), coord(2));childmat(x,y) = 1 - childmat(x,y);
    end
end
```

Python实现方法, 在python中实现这个可以使用**取模运算(%)**

```python
def mutation_nsew(matrix):
    """
    基于 PPT 描述的 '东南西北环绕式领域交换' 变异算子。
    """
    rows, cols = matrix.shape
    mutated_matrix = matrix.copy()
    
    # 1. 随机选择一个中心点
    x = np.random.randint(0, rows)
    y = np.random.randint(0, cols)
    
    # 2. 定义东南西北四个方向的偏移量 (dx, dy)
    directions = [
        (-1, 0), # North
        (1, 0),  # South
        (0, -1), # West
        (0, 1)   # East
    ]
    
    # 为了增加随机性，打乱尝试方向的顺序
    np.random.shuffle(directions)
    
    # 3. 遍历邻居并尝试交换
    for dx, dy in directions:
        # 计算邻居坐标，使用取模运算 (%) 实现环绕/穿墙效果
        # 例如：如果 x=-1，python 的 -1 % rows 会自动变成 rows-1 (最后一行)
        nx = (x + dx) % rows
        ny = (y + dy) % cols
        
        # 4. 如果中心点和邻居点的值不同 (即一个是0，一个是1)
        if mutated_matrix[x, y] != mutated_matrix[nx, ny]:
            # 交换两者
            mutated_matrix[x, y], mutated_matrix[nx, ny] = \
                mutated_matrix[nx, ny], mutated_matrix[x, y]
            
            # 变异通常只发生一次即可，交换成功后立即退出
            break
            
    return mutated_matrix
```


**交叉**：**模板化全局交换.** 设矩阵A和B是被选出来进行交换的两个矩阵。首先，将矩阵的元素按行的顺序依次头为相接地排成**一维数组**，并将数组地头尾连成环绕地形式。然后，随机选出一个共同的起点位置，沿着两个数组的元素**向后移动直到第一次出现两个不同的元素**，则交换这两个元素，继续沿着数组的元素移动，不断地及进行类似的交换元素，这个过程直至遇到第一次出现不同元素的位置时结束。


Matlab实现方法

```matlab
function newmat = crossover(mat1, mat2)
[m, n] = size(mat1); % 获取矩阵尺寸
% 一种实现方式，可向量化优化之
for i = 1:m 
    for j = 1:n % 遍历矩阵每一个格子
        if mat1(i, j) ~= mat2(i, j) % 检查两个矩阵在同一位置的值是否不同
            mat1(i, j) = 1 - mat1(i, j);
            mat2(i, j) = 1 - mat2(i, j); % 这是 MATLAB 中常用的位翻转技巧。
            % 如果是 0，1-0=1；如果是 1，1-1=0。这行代码实现了值的交换
        end
    end
end
newmat = mat1;
end
```


<img src="https://zzh123-1325455460.cos.ap-nanjing.myqcloud.com/202511211055949.png" width="400"/>


Python实现方法

```python
import numpy as np

def crossover_conserved(parent_a, parent_b):
    """
    基于 PPT 描述的 '模板化全局交换' 交叉算子。
    核心逻辑：在保持 1 的总数不变的前提下交换基因。
    """
    # 复制父代，避免修改原始数据
    child_a = parent_a.copy()
    child_b = parent_b.copy()
    
    rows, cols = parent_a.shape
    # 1. 扁平化：将矩阵展平为一维数组
    flat_a = child_a.flatten()
    flat_b = child_b.flatten()
    total_len = len(flat_a)
    
    # 2. 随机起点
    start_idx = np.random.randint(0, total_len)
    
    # 记录 '1' 的盈亏情况。
    # balance > 0 表示 A 多了 1 (B 少了 1)
    # balance < 0 表示 A 少了 1 (B 多了 1)
    balance = 0 
    
    # 3. 环绕遍历 (最多遍历一圈)
    for k in range(total_len):
        # 环绕索引
        idx = (start_idx + k) % total_len
        
        # 如果两个位置元素不同，尝试交换
        if flat_a[idx] != flat_b[idx]:
            # 执行交换
            flat_a[idx], flat_b[idx] = flat_b[idx], flat_a[idx]
            
            # 更新平衡状态
            # 如果 flat_a 现在变成了 1 (说明原来是 0)，balance + 1
            # 如果 flat_a 现在变成了 0 (说明原来是 1)，balance - 1
            if flat_a[idx] == 1:
                balance += 1
            else:
                balance -= 1
            
            # 4. 终止条件：如果平衡回归为 0，且不仅仅是因为刚开始，说明完成了一次闭环交换
            if balance == 0:
                break
    
    # 将一维数组重新塑形回矩阵
    return flat_a.reshape(rows, cols), flat_b.reshape(rows, cols)
```

算例主程序

种群参数

```matlab
popnum = 12; % 种群大小 (Population Size)。每一代有 12 个矩阵个体。
n = 3; % 阶数
m = 5; % 1的个数
maxGen = 1000;
perm_GA(popnum,n,m,maxGen); % 最大迭代次数。繁衍 1000 代后停止。
```

遗传过程

```matlab
% 轮盘赌选择(实际上是锦标赛选择, 随机选几个人出来打架，最强的那个胜出)
tournamentSize = 4; %设置大小
for k=1:popnum % 生成新一代 , 我们要生成与上一代数量相同的新个体
    % 选择父代
    tourPopDistances = zeros(tournamentSize, 1); % 建立一个临时名单，用来存参赛选手的成绩。
    % 随机从种群中抽取 4 个幸运观众（randi(popnum) 生成随机索引）
    % 并把他们的适应度（存放在 B 数组中）记录下来
    for i = 1:tournamentSize
        randomRow = randi(popnum);tourPopDistances(i,1) = B(randomRow);
    end 
    % 选择最好的A, 找出这 4 个人里适应度最高的那个数值
    parent1 = max(tourPopDistances);
    % 根据这个最高适应度值，去原始种群矩阵库 mat 中把对应的矩阵实体取出来
    parent1mat = mat(:, :, find(B==parent1, 1)); 
    for i = 1:tournamentSize
        randomRow = randi(popnum);
        tourPopDistances(i,1) = B(randomRow);
    end
    % 选择最好的B
    parent2 = max(tourPopDistances);
    parent2mat = mat(:, :, find(B==parent2, 1)); 
    % 执行交叉
    submat = crossover(parent1mat, parent2mat);
    % 执行变异
    submat = mutate(submat); 
    % 获得新的一代
    offspring(:, :, k) = submat;
end
```

#### 10.2.3 粘滞现象及修正

**粘滞现象 (早熟现象)** : 遗传算法在迭代过程中，适应度的改善速度会越来越慢，最终 “陷入” 局部最优解（而不是找到全局最优），这就是粘滞 / 早熟现象。

**修正方法**: 核心思路是**扩大交叉操作的群体范围**，打破当前的局部最优束缚; 让当前群体的状态，和整个可选状态空间里的任意 / 随机状态进行交叉，而不是只和当前群体内的个体交叉。引入更多多样性，帮助算法跳出局部最优，向全局最优靠近。

<img src="https://zzh123-1325455460.cos.ap-nanjing.myqcloud.com/202511211459916.png" width="400"/>

#### 10.2.4 模式定理 (Schema Theorem)

<span style="font-size: 20px; color: #4F845C;">模式定理是遗传算法的核心理论，它解释了 “优秀模式” 在遗传迭代中会指数级增长的规律. </span>


- **模式（Schema）**：是遗传算法中字符串（个体）的 “相似片段”，可以理解为一种具有共性特征的基因片段。
- **确定位数（阶）**：模式中固定字符的数量（越少越好）；
- **定义长度**：模式中第一个和最后一个固定字符的距离（越短越好）；
- **适应度**：模式对应的个体的平均适应度。

在遗传算法的选择、交叉、变异操作作用下, 
<center><strong>确定位数少、定义长度短、且平均适应度高于种群平均的模式（即 “优秀模式”），会在子代种群中呈指数级增长。</center></strong>

**模式定理的证明:**

先明确符号定义
- $ m(H,t) $：第$ t $代中，属于模式$ H $的个体数量；
- $ p_i = \frac{f_i}{\sum_{i=1}^n f_i} $：个体$ A_i $被选择的概率（适应度越高，概率越大）；
- $ \bar{f} = \frac{\sum_{i=1}^n f_i}{n} $：种群的平均适应度；
- $ f(H) $：模式$ H $对应的所有个体的平均适应度。

选择操作后模式 $H$ 的数量变化. 当用新种群（非重叠的$ n $个串）代替旧种群时：
每个属于模式$ H $的个体，被选中的概率是$ \frac{f(H)}{\sum_{i=1}^n f_i} $（因为模式$ H $的平均适应度是$ f(H) $）。
因此，第$ t+1 $代中模式$ H $的数量为：

$$
m(H,t+1) = m(H,t) \times n \times \frac{f(H)}{\sum_{i=1}^n f_i}
$$

又因为$ \bar{f} = \frac{\sum_{i=1}^n f_i}{n} $，即$ \sum_{i=1}^n f_i = n\bar{f} $，代入后化简得：

$$
m(H,t+1) = m(H,t) \times \frac{f(H)}{\bar{f}}
$$

假设模式$ H $的适应度比种群平均高$ c\bar{f} $（即$ f(H) = (1+c)\bar{f} $，$ c>0 $代表“优秀”），代入上式：

$$
m(H,t+1) = m(H,t) \times \frac{(1+c)\bar{f}}{\bar{f}} = m(H,t) \times (1+c)
$$

以此类推，迭代$ t $代后，模式$ H $的数量会变成：

$$
m(H,t) = (1+c)^t \times m(H,0)
$$

这就证明了： **适应度高于种群平均的模式，在选择操作下会呈指数级增长** 。


## 神经网络模拟


> 还没上呢


## 扩散采样


> 还没上呢


## KMC -- 分子动力学



> 还没上呢

## SOE

> 还没上呢

## 复杂网络与元胞自动机


> 还没上呢
