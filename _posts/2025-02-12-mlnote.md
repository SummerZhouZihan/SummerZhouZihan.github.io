---
layout:     post
title:      "Welcome to Machine Learning"
subtitle:   " \"Machine Learning Note\""
date:       2025-01-12 11:00:00
author:     "Summer"
header-img: "img/post-bg-2015.jpg"
tags:
    - 机器学习
---
# 第一部分 监督学习:线性回归与逻辑回归(Supervised Machine Learning: Regression and Classification)

- ### )

[TOC]

## 0 一些有趣的话
- 你学明白了吗,就让机器学?
- garbage in garbage out

## 1 机器学习绪论
### 1.1 what is machine learning
机器学习的定义：field of study that gives computers the ability to learn without being explicitly programmed 在没有明确编程的情况下学习
- 目前常用的两种机器学习算法：
- 监督学习（Supervised learning）
- 无监督学习（Supervised learning）

Reinforcement learning 强化学习，是另一种机器学习

### 1.2 监督学习(Supervised learning)

FIGURE:learn from given "right answer"
input(x) $\rightarrow$ output label(y)
最终接受只有输入没有输出（“right answer”），给出合理预测
- 回归算法(regression algorithms),例如根据房价预测；回归试图预测无限多可能的数字中的任意一个
- 分类算法(classification algorithms)，例如breast cancer detection
  恶性的肿瘤malignant，还是良性的benign；分类只有n种可能的输出,predict categories,是猫还是狗.需要找到分类的边界
  
    > 案例:given email labeled as spam/not spam,learn a spam filter

### 1.3 无监督学习(Unsupervised learning)
定义:data only comes with inputs x, but not output labels y
Algorithm has to find structure in the data 

给定的数据与任何输出标签y无关，我们的工作是找到一些结构或者模式，或者只是在数据中找到有趣的东西

- 类聚算法(clustering algorithm),将未标记的数据放入不同的集群中;在没有监督的情况下寻找分类,group similar data points together.
>- 案例1：given a set of news articles found on the web,group them into sets of articles about the same story.熊猫的新闻中出现了panda,zoo,japan等等词，类聚算法会将他们自动查找并归类；一天有很多新闻，算法可以无监督地寻找归类cluster。

>- 案例2:given a database of customer data,automatically discover market segments and group customers into different market segments

- 异常检测(Anomaly detection):find unusual data points


- 降维(Dimensionality reduction):将大数据集压缩维小数据集同时尽可能丢失少的信息 

### 1.4 Jupiter notebook

运行代码：按住 Shift 键并按 'Enter' 

## 2 单变量线性回归
专有名词
|单词|释义|单词|释义
|----------|--------|----------|--------|
|hypothesis	|假设函数|	Linear regression	|线性回归|
|Parameter	|模型参数	|cost function	|代价函数|
|Gradient descent	|梯度下降	|convex function|凸函数|

### 2.1 线性回归模型(Linear Regression Model)


数据集被称作训练集(data set)
Notation:
- m = Number of training examples →训练样本的数量
- x’s = “input” variable / features →输入变量 / 特征
- y’s = “output” variable / “target” variable →输出变量 / 目标变量
- (x, y) = single training example →一个训练样本
- ($ x^{(i)}$,$ y^{(i)}$) = ith training example →第i个训练样本
### 2.2 Cost function 成本/损失函数

Model模型：$$ f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot \mathbf{x}^{(i)} + b  $$ 
Parameter参数: $$ \mathbf{w},\mathbf{X}$$
Cost function成本函数：
$$J(\mathbf{w},b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})^2 $$ 
Objective目标: 使成本函数$J(\mathbf{w},b)$最小

### 2.3 Gradient descent 梯度下降
#### 2.3.1 什么是梯度算法
梯度下降是用来找到$J(\mathbf{w},b)$最小值的算法
1. 随机从$w$和$b$的某个值出发
2. 一步一步下山直至收敛到局部最低点
3. 梯度下降算法的特点：不同的起始点出发会到达不同的局部最小值(local minima)
#### 2.3.2 梯度算法表达式
下面我们来实现梯度下降(Gradient descent)的算法
多个变量的梯度算法：(重复执行直到满足收敛条件为止)
$$\begin{align*} \text{repeat}&\text{ until convergence:} \; \lbrace \newline\;
& w_j = w_j -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j}  \; & \text{for j = 0..n-1}\newline
&b\ \ = b -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial b}  \newline \rbrace
\end{align*}$$

在这个等式中，$\alpha$是学习率(learning rate)，通常是0~1之间的小正数。如果$\alpha$非常大，说明正在采取非常激进的下坡方式，每一步迈得很大。
事实上，学习率不能过小或过大。
- 如果学习率过小，每一步都很小，程序会很慢
- 如果学习率过大，程序一步就会迈过极小值，会越来越发散

#### 2.3.3 Simultaneous update 同步更新

正确的更新方式：

> $$\begin{align*}   
& temp_w = w_j -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j}  \; & \text{for j = 0..n-1}\newline
&temp_b \ = b -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial b}  \newline
&  w = temp_w  \newline
& b = temp_b 
\end{align*}$$

下面这个是错误的更新方式：
> $$\begin{align*}   
& temp_w = w_j -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j}  \; & \text{for j = 0..n-1}\newline
&  w = temp_w  \newline
&temp_b \ = b -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial b}  \newline
& b = temp_b 
\end{align*}$$

1. 更新方程时需要同时更新$w$和$b$
2. 正确方法：先同时计算右边部分，然后同时更新$w$和$b$
3. $\times$错误方法：先计算temp0然后更新θ_0，再计算temp1然后更新θ_1

### 2.4 Gradient descent for linear regression线性回归中的梯度下降算法
- 线性回归中不会出现多个极小值，三维图像永远是碗形，即convex function
- bashed gradient descent指的是在梯度下降的每一步中，我们都在查看所有的训练事例，使用了整个训练集（因为有求和）

## 3 线性代数复习
### 3.1 矩阵

1. 矩阵(Matrix)的定义:
    Rectangular array of numbers 由数字组成的矩形阵列
2. 矩阵的维度(dimension):
    number of rows x number of columns 矩阵的行数乘以列数



### 3.2 矩阵加法
设 $A = (a_{ij})$ 和 $B = (b_{ij})$ 是两个 $m\times n$ 矩阵，则它们的和 $C = A + B$ 也是一个 $m\times n$ 矩阵，其中 $c_{ij}=a_{ij}+b_{ij}$。

例如，若
\[
A=\begin{pmatrix}
a_{11}&a_{12}\\
a_{21}&a_{22}
\end{pmatrix}
\]
\[
B=\begin{pmatrix}
b_{11}&b_{12}\\
b_{21}&b_{22}
\end{pmatrix}
\]

则
\[
A + B=\begin{pmatrix}
a_{11}+b_{11}&a_{12}+b_{12}\\
a_{21}+b_{21}&a_{22}+b_{22}
\end{pmatrix}
\]
- 矩阵加法满足交换律 $A + B = B + A$ 和结合律 $(A + B)+C = A+(B + C)$。

### 3.3 标量乘法
设 $k$ 是一个标量，$A=(a_{ij})$ 是一个 $m\times n$ 矩阵，则标量乘法 $kA$ 是一个 $m\times n$ 矩阵，其中元素为 $(kA)_{ij}=k\times a_{ij}$。

例如，若
\[
A=\begin{pmatrix}
a_{11}&a_{12}\\
a_{21}&a_{22}
\end{pmatrix}
\]

则
\[
kA=\begin{pmatrix}
ka_{11}&ka_{12}\\
ka_{21}&ka_{22}
\end{pmatrix}
\]

- 标量乘法满足结合律 $k(lA)=(kl)A$ 和分配律 $k(A + B)=kA + kB$。

### 3.4 矩阵的积
设 $A$ 是一个 $m\times n$ 矩阵，$B$ 是一个 $n\times p$ 矩阵，记 $A=(a_{ij})$ 和 $B=(b_{ij})$，则矩阵 $C = AB$ 是一个 $m\times p$ 矩阵，其中元素 $c_{ij}=\sum_{k = 1}^{n}a_{ik}b_{kj}$。

例如，若
\[
A=\begin{pmatrix}
a_{11}&a_{12}\\
a_{21}&a_{22}
\end{pmatrix}
\]
\[
B=\begin{pmatrix}
b_{11}&b_{12}\\
b_{21}&b_{22}
\end{pmatrix}
\]

则
\[
AB=\begin{pmatrix}
a_{11}b_{11}+a_{12}b_{21}&a_{11}b_{12}+a_{12}b_{22}\\
a_{21}b_{11}+a_{22}b_{21}&a_{21}b_{12}+a_{22}b_{22}
\end{pmatrix}
\]
- 矩阵乘法满足结合律 $(AB)C = A(BC)$ 和分配律 $A(B + C)=AB + AC$，但一般不满足交换律 $AB\neq BA$。

### 3.5 矩阵的逆
对于一个 $n\times n$ 的方阵 $A$，如果存在一个 $n\times n$ 的方阵 $B$，使得 $AB = BA = I$，其中 $I$ 是 $n\times n$ 的单位矩阵，则称 $B$ 是 $A$ 的逆矩阵，记为 $A^{-1}=B$。

并非所有方阵都有逆矩阵，一个方阵 $A$ 可逆的充分必要条件是其行列式 $\vert A\vert\neq 0$。

例如，对于 $2\times 2$ 矩阵
\[
A=\begin{pmatrix}
a&b\\
c&d
\end{pmatrix}
\]
其逆矩阵为
\[
A^{-1}=\frac{1}{ad - bc}\begin{pmatrix}
d&-b\\
-c&a
\end{pmatrix}
\]，前提是 $ad - bc\neq 0$。

- 只有方阵才可能有逆矩阵，并且可逆的方阵满足 $AA^{-1}=A^{-1}A = I$。

### 3.6 转置矩阵
设 $A=(a_{ij})$ 是一个 $m\times n$ 矩阵，则其转置矩阵 $A^T=(a_{ji})$ 是一个 $n\times m$ 矩阵。

例如，若
\[
A=\begin{pmatrix}
a_{11}&a_{12}\\
a_{21}&a_{22}
\end{pmatrix}
\]

则
\[
A^T=\begin{pmatrix}
a_{11}&a_{21}\\
a_{12}&a_{22}
\end{pmatrix}
\]

- 转置矩阵具有性质 $(A^T)^T = A$，$(A + B)^T = A^T + B^T$，$(kA)^T = kA^T$，$(AB)^T = B^TA^T$。


## 4 Multivariate linear regression 多元线性回归

名词翻译

|单词|词义|单词|词义|
|--------|--------|--------|--------|
|feature	|特征	|Multivariate linear regression |多元线性回归 	|
|Feature Scaling	|特征缩放	|Example automatic convergence test	|自动收敛测试|
|Polynomial regression	|多项式回归	|Normal equation	|正规方程|

### 4.1 Multiple features 多元线性回归
多元线性回归：即有多个特征的线性回归
- n = number of features →训练样本的数量
- $\overrightarrow{x}^{(i)}$ = input features of ith training example →第i个训练样本的输入特征值,在多元特征中，$\overrightarrow{x}^{(i)}$是一个一维向量，即一行多列
- $x_j^{(i)}$ = value of feature j in ith training example →第i个训练样本中第j个特征值

### 4.2 多元线性回归的梯度下降
#### 4.2.1 常规解法
$$\begin{align*}\text{Parameters }&\overrightarrow{w} = [w_1 \ w_2 \ ...\  w_n]
\newline &b\text{ \   still a number}
\newline \text{model  }& f_{\overrightarrow{w},b}(\mathbf{x})=\overrightarrow{w}\cdot\overrightarrow{x}+b
\newline \text{cost function \    } &J(\overrightarrow{w},b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{\overrightarrow{w},b}(\mathbf{x}^{(i)}) - y^{(i)})^2 
\newline \text{Gradient decent }& w_j = w_j -  \alpha \frac{\partial J(\overrightarrow{w},b)}{\partial w_j}  \; \text{for j = 0..n-1}
\newline & b = b -  \alpha \frac{\partial J(\overrightarrow{w},b)}{\partial b}
\end{align*}$$

#### 4.2.2 Normal equation 正规方程 (只适用于线性回归)
- 提供了一个求最小值的解析解法
- 正规方程不需要学习率，不用迭代
- 正规方程速度较慢

### 4.3 Feature Scalling 特征缩放
#### 4.3.1 特征缩放可以使梯度更快收敛
当一个案例有不同的features,他们具有不同的值的范围，会导致梯度下降运行缓慢
> 例如，在房价预测中，房间面积$x_1$为2000，房间数量$x_2$为5，最终价格300k
> 参数$w_1=0.05$，参数$w_2=50$，参数$b=50$，等高线是椭圆，做梯度下降会运行缓慢
> 假如对$x_1 ,x_2$做缩放，使得范围相近，等高线变为圆形，加快梯度下降

#### 4.3.2 Mean normalization 均值归一化

##### 1.除以最大值
$x_1$在区间[a,b]中,$x_2$在区间[c,d]中
归一化后，$\widehat{x_1}=\frac{x_1}{b}$，$\widehat{x_2}=\frac{x_2}{b}$
##### 2. Mean normalization
$x_1$在区间[a,b]中,$x_2$在区间[c,d]中
计算得到$x_1$的平均值$\mu_1$，$x_2$的平均值$\mu_2$
归一化后，$\widehat{x_1}=\frac{x_1-\mu_1}{b-a}$,$\widehat{x_2}=\frac{x_2-\mu_2}{d-c}$
此时$\widehat{x_1}$和$\widehat{x_2}$在(-1,1)内部
##### 3.  Z-score normalization z分数归一化
求出平均值$\mu_1$和标准差(standard deviation)$\sigma_1$
归一化后，$$\widehat{x^{(i)}_j} = \dfrac{x^{(i)}_j - \mu_j}{\sigma_j} $$ 
其中，$$
\begin{align*}
\mu_j &= \frac{1}{m} \sum_{i=0}^{m-1} x^{(i)}_j \newline
\sigma^2_j &= \frac{1}{m} \sum_{i=0}^{m-1} (x^{(i)}_j - \mu_j)^2 
\end{align*}
$$
注意${x^{(i)}_j}$表示第i个训练样本的第j个特征

在z分数归一化后,所有要素的平均值为0,标准差为1



### 4.4 检查梯度下降是否收敛
方法一:画一张图
方法二:Automatic convergence test
令$\epsilon = 10^{-3}$
如果在一次迭代中，$J(\overrightarrow{w},b) \leq \epsilon$
那么可以认为收敛

### 4.5 选择合适的学习率

![4.5 选择合适的学习率](image.png)
一个debug小技巧,把$\alpha$设置的很小,来检查成本函数是否会下降
如果此时成本函数仍然上升,说明程序中有bug
反复尝试0.001,0.01,0.1,1等数值,选择下降较快的学习率

### 4.6 Feature Engineering 特征工程
可以根据现实情况,组合原本的特征
> 例如在房价预测中,地块长度$x_1$,宽度$x_2$,原本的函数为
> $$
f_{\overrightarrow{w},b}=w_1x_1+w_2x_2+b
> $$
> 然而我们知道地块面积$x_3=x_1x_2$也与房价有关,函数变为
> $$
f_{\overrightarrow{w},b}=w_1x_1+w_2x_2+w_3x_3+b
> $$
> 这就是特征工程

### 4.7 多项式回归(Polynomial regression)



### 4.8 利用Scikit-Learn进行回归

#### 4.8.1 线性回归模型(LinearRegression)

```
# 导入numpy库和sklearn库
import numpy as np
from sklearn.linear_model import LinearRegression

# 导入数据
X_train = np.array([[1], [2], [3], [4], [5]])
y_train = np.array([2, 4, 6, 8, 10])

# 创建并训练线性回归模型
model = LinearRegression()
model.fit(X_train, y_train)

# 待预测的数据
X_test = np.array([[6], [7]])

# 使用predict函数进行预测
y_pred = model.predict(X_test)
print("预测结果:", y_pred)
```

#### 4.8.2 逻辑回归模型(LogisticRegression)
```
import numpy as np
from sklearn.linear_model import LogisticRegression

# 训练数据
X_train = np.array([[1], [2], [3], [4], [5]])
y_train = np.array([0, 0, 1, 1, 1])

# 创建并训练逻辑回归模型
model = LogisticRegression()
model.fit(X_train, y_train)

# 待预测的数据
X_test = np.array([[6], [7]])

# 进行预测
y_pred = model.predict(X_test)
print("预测结果:", y_pred)

```


#### 4.8.3 决策树模型(DecisionTreeClassifier)
```
import numpy as np
from sklearn.tree import DecisionTreeClassifier

# 训练数据
X_train = np.array([[1], [2], [3], [4], [5]])
y_train = np.array([0, 0, 1, 1, 1])

# 创建并训练决策树分类器
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# 待预测的数据
X_test = np.array([[6], [7]])

# 进行预测
y_pred = model.predict(X_test)
print("预测结果:", y_pred)
```

## 5 逻辑回归
逻辑回归的特点:解决二进制问题,输出标签y只有0或者1
### 5.1 逻辑回归的公式
逻辑回归的公式也叫sigmoid函数
$$g(z) = \frac{1}{1+e^{-z}}$$
在 Logistic 回归的情况下，z（sigmoid 函数的输入）是线性回归模型的输出。
- 在单个示例的情况下,z是 scalar。
- 在多个示例的情况下,z可能是由m个值组成的向量，每个示例一个值。
python代码实现:
```
def sigmoid(z):
    """
    Compute the sigmoid of z

    Args:
        z (ndarray): A scalar, numpy array of any size.

    Returns:
        g (ndarray): sigmoid(z), with the same shape as z
         
    """

    g = 1/(1+np.exp(-z))
   
    return g

```

### 5.2 决策边界(Decision boundary)
==决策边界实质上是函数输入z等于0时的曲线==
![决策边界](image-1.png)
对于函数
$$g(z) = \frac{1}{1+e^{-z}}$$
and $z=\mathbf{w} \cdot \mathbf{x}$ is the vector dot product:


如果 $f_{\mathbf{w},b}(x) >= 0.5$, 预测 $y=1$
  
如果 $f_{\mathbf{w},b}(x) < 0.5$, 预测 $y=0$


### 5.3 逻辑回归的成本函数
![逻辑回归成本函数1](image-2.png)
如果使用平方成本函数,会有很多局部最小值
我们需要一个新的成本函数可以使得函数再次变为凸的
==**逻辑回归的损失(loss)函数:**==
$$
\begin{equation}
  loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) = \begin{cases}
    - \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) & \text{if $y^{(i)}=1$}\\
    \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) & \text{if $y^{(i)}=0$}
  \end{cases}
\end{equation}
$$
- 等式中$f_{\mathbf{w},b}(\mathbf{x}^{(i)})$是模型的预测,$y^{(i)}$是模型的目标值

- 其中$f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(\mathbf{w} \cdot\mathbf{x}^{(i)}+b)$ ,在这里 $g$ 是逻辑回归函数(sigmoid function)
- $f_{\mathbf{w},b}(\mathbf{x}^{(i)})$距离$y^{(i)}$越远,损失越大,损失函数越大

或者,损失函数还可以写成一个等式
$$loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)$$
注意在这个等式中 , $y^{(i)}$只能取到0或1

==**逻辑回归的成本(cost)函数**==
$$ J(\mathbf{w},b) = \frac{1}{m} \sum_{i=0}^{m-1} \left[ loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) \right] $$
其中:

* $loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)})$ 是单个数据点的损失函数, 也就是:

    $$loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) = -y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)$$
    
*  其中 `m` 是数据集中的训练样本数,并且:
$$
\begin{align*}
f_{\mathbf{w},b}(\mathbf{x^{(i)}}) &= g(z^{(i)}) \\
z^{(i)} &= \mathbf{w} \cdot \mathbf{x}^{(i)}+ b \\
g(z^{(i)}) &= \frac{1}{1+e^{-z^{(i)}}}
\end{align*} 
$$

python代码实现
```
def compute_cost_logistic(X, y, w, b):
    """
    Computes cost

    Args:
      X (ndarray (m,n)): Data, m examples with n features
      y (ndarray (m,)) : target values
      w (ndarray (n,)) : model parameters  
      b (scalar)       : model parameter
      
    Returns:
      cost (scalar): cost
    """

    m = X.shape[0]
    cost = 0.0
    for i in range(m):
        z_i = np.dot(X[i],w) + b
        f_wb_i = sigmoid(z_i)
        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)
             
    cost = cost / m
    return cost
```

### 5.4 逻辑回归的梯度下降

==逻辑回归的梯度下降公式:==
$$\begin{align*}
&\text{重复直至收敛:} \; \lbrace \\
&  \; \; \;w_j = w_j -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j}   \; & \text{for j := 0..n-1} \\ 
&  \; \; \;  \; \;b = b -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial b} \\
&\rbrace
\end{align*}$$

其中，每次迭代对所有不同$j$的$w_j$同时更新, 在这里
$$\begin{align*}
\frac{\partial J(\mathbf{w},b)}{\partial w_j}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  \\
\frac{\partial J(\mathbf{w},b)}{\partial b}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})  
\end{align*}$$

* `m` 是数据集中的训练样本数      
* $f_{\mathbf{w},b}(x^{(i)})$ 是模型的预测, while $y^{(i)}$ 是目标
* 在 Logistic 回归模型中 
    $z = \mathbf{w} \cdot \mathbf{x} + b$  
    $f_{\mathbf{w},b}(x) = g(z)$  
    这里的 $g(z)$ 是sigmoid函数:  
    也就是$g(z) = \frac{1}{1+e^{-z}}$   

python代码实现
```
def compute_gradient_logistic(X, y, w, b): 
    """
    Computes the gradient for linear regression 
 
    Args:
      X (ndarray (m,n): Data, m examples with n features
      y (ndarray (m,)): target values
      w (ndarray (n,)): model parameters  
      b (scalar)      : model parameter
    Returns
      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. 
      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. 
    """
    m,n = X.shape
    dj_dw = np.zeros((n,))             #(n,)
    dj_db = 0.

    for i in range(m):
        f_wb_i = sigmoid(np.dot(X[i],w) + b)       #(n,)(n,)=scalar
        err_i  = f_wb_i  - y[i]            #scalar
        for j in range(n):
            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar
        dj_db = dj_db + err_i
    dj_dw = dj_dw/m             #(n,)
    dj_db = dj_db/m                                   #scalar
        
    return dj_db, dj_dw  
```

### 5.5 使用scikit-learn完成逻辑函数回归
```
# 引入数据集
import numpy as np

X = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])
y = np.array([0, 0, 0, 1, 1, 1])
# 训练模型
from sklearn.linear_model import LogisticRegression

lr_model = LogisticRegression()
lr_model.fit(X, y)
# 进行预测
y_pred = lr_model.predict(X)

print("Prediction on training set:", y_pred)
#检查精度
print("Accuracy on training set:", lr_model.score(X, y))
```




## 6 过拟合 与 正则化
### 6.1 过拟合
![6.1 过拟合](/img/in-post/post-mlnote/image-4.png)
- 未拟合(underfit):不能很好地适应训练集,有较大的偏差(high bias)
- 恰到好处(just right)
- 过拟合(overfit):有较大的方差(high variance)

![分类过拟合](/img/in-post/post-mlnote/image-5.png)

解决方法:
1. 收集更多的数据.适合有大量数据的情况
2. 挑选合适的特征.适合数据量较少,特征量较多的情况.缺点是只使用了训练集的一部分数据,丢掉了另一部分的数据.
3. 正则化(Regularization),减小参数,防止某一个特征产生过大的影响

### 6.2 正则化(Regulation)
![alt text](/img/in-post/post-mlnote/image-6.png)

先建立101个特征的模型(100个w和b),因为我们不知道哪些需要缩小;一般情况下我们不会缩小b

$$J(\mathbf{w},b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})^2  + \frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2 $$ 


1. 在成本函数加一个额外的正则化项,目的是缩小每一个参数的值
2. 一般不给$b$增加惩罚项，无论是否包括$b$实际上对结果影响都不大
3. λ称为正则化参数,可以控制两个不同目标之间的取舍,进一步控制两项的平衡关系
    1. 第一个目标（与第一项有关）：更好地拟合训练集数据
    2. 第二个目标（与正则化项有关）：保持参数尽量的小
4. 如果λ被设的太大的话，参数的惩罚程度过大，参数都会接近于0 →$h_w(x)$ = $w_0$ →欠拟合
5. 因此,需要选择一个合适的正则化参数λ
#### 6.2.1 正则化的成本函数

==正则化的成本函数是:==
$$J(\mathbf{w},b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})^2  + \frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2 $$ 
在这里:
$$ f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot \mathbf{x}^{(i)} + b  $$ 


回顾一下,未正则化的是:

$$J(\mathbf{w},b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})^2 $$ 

存在差异的项是,  <span style="color:blue">
    $$\frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2$$ </span> 
    
#### 6.2.2 正则化的逻辑回归成本函数

正则化的**逻辑回归**成本函数
$$J(\mathbf{w},b) = \frac{1}{m}  \sum_{i=0}^{m-1} \left[ -y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \right] + \frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2 $$
在这里:
$$ f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = sigmoid(\mathbf{w} \cdot \mathbf{x}^{(i)} + b)  $$ 

回顾一下未正则化的表达式:

$$ J(\mathbf{w},b) = \frac{1}{m}\sum_{i=0}^{m-1} \left[ (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)\right] $$

差异项是    <span style="color:blue">
    $$\frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2$$ </span> 

```
def compute_cost_logistic_reg(X, y, w, b, lambda_ = 1):
    """
    Computes the cost over all examples
    Args:
    Args:
      X (ndarray (m,n): Data, m examples with n features
      y (ndarray (m,)): target values
      w (ndarray (n,)): model parameters  
      b (scalar)      : model parameter
      lambda_ (scalar): Controls amount of regularization
    Returns:
      total_cost (scalar):  cost 
    """

    m,n  = X.shape
    cost = 0.
    for i in range(m):
        z_i = np.dot(X[i], w) + b                                      #(n,)(n,)=scalar, see np.dot
        f_wb_i = sigmoid(z_i)                                          #scalar
        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)      #scalar
             
    cost = cost/m                                                      #scalar

    reg_cost = 0
    for j in range(n):
        reg_cost += (w[j]**2)                                          #scalar
    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar
    
    total_cost = cost + reg_cost                                       #scalar
    return total_cost                                                  #scalar
```


#### 6.2.2 正则化的梯度下降(线性/逻辑)
重新定义函数$J_{(w,b)}$后,照常操作梯度下降

线性和逻辑回归的梯度下降是一样的,只是$f_{\mathbf{w},b}$的计算不同.
$$\begin{align*}
\frac{\partial J(\mathbf{w},b)}{\partial w_j}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \frac{\lambda}{m} w_j \tag{1} \\
\frac{\partial J(\mathbf{w},b)}{\partial b}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)}) \tag{2} 
\end{align*}$$

* m 是数据集中的训练样本数     
* $f_{\mathbf{w},b}(x^{(i)})$ 是模型的预测, $y^{(i)}$ 是目标

* 在  <span style="color:blue"> **线性** </span> 回归模型中
    $f_{\mathbf{w},b}(x) = \mathbf{w} \cdot \mathbf{x} + b$  
* 在 <span style="color:blue"> **逻辑** </span> 回归模型中
    $z = \mathbf{w} \cdot \mathbf{x} + b$  
    $f_{\mathbf{w},b}(x) = g(z)$  
    where $g(z)$ is the sigmoid function:  
    $g(z) = \frac{1}{1+e^{-z}}$   
    

正则化后多出来的项是 <span style="color:blue">$$\frac{\lambda}{m} w_j $$</span>.

下面我们尝试深入理解正则化后的$w_j$式子
移项后我们发现
$$\begin{align*}
w_j  &=   w_j (1-\alpha\frac{\lambda}{m} ) - \alpha\frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}
\end{align*}$$

其中第二项是通常的更新项
$\alpha$通常是0.01左右,$\lambda$通常是1-10,假设m为50,那么$\alpha\frac{\lambda}{m}$为0.0002.因此我们知道$\alpha\frac{\lambda}{m}$一般是大于0的小正数,因此$(1-\alpha\frac{\lambda}{m} )$通常是接近1但小于1的正数.每一次迭代$w_j$都会减小一点

```
def compute_gradient_linear_reg(X, y, w, b, lambda_): 
    """
    Computes the gradient for linear regression 
    Args:
      X (ndarray (m,n): Data, m examples with n features
      y (ndarray (m,)): target values
      w (ndarray (n,)): model parameters  
      b (scalar)      : model parameter
      lambda_ (scalar): Controls amount of regularization
      
    Returns:
      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. 
      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. 
    """
    m,n = X.shape           #(number of examples, number of features)
    dj_dw = np.zeros((n,))
    dj_db = 0.

    for i in range(m):                             
        err = (np.dot(X[i], w) + b) - y[i]                 
        for j in range(n):                         
            dj_dw[j] = dj_dw[j] + err * X[i, j]               
        dj_db = dj_db + err                        
    dj_dw = dj_dw / m                                
    dj_db = dj_db / m   
    
    for j in range(n):
        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]

    return dj_db, dj_dw
```

# 第二部分 高级学习算法(Advanced Learning Algorithms)
![第二部分概要](img/in-post/post-mlnote/image-7.png)
为什么深度学习在这两年迅速兴起
![深度学习与机器学习的曲线](img/in-post/post-mlnote/image-8.png)

## 1 神经网络
![alt text](img/in-post/post-mlnote/image-9.png)
input layer - hidden layer - output layer
这个案例中的神经网络有两层,其中中间的一层叫隐藏层(hidden layer),最后一层叫输出层.第零层一般是输入层.

### 1.1 神经网络的层(layer)
![第一层](img/in-post/post-mlnote/image-10.png)

$a^{[1]}$是第一层的输出,为了保证不混淆,可以把第一层中的参数上标为$[1]$,表示是第一层的参数
![第二层](img/in-post/post-mlnote/image-11.png)
第二层输出的上标是$[2]$.第三层就是逻辑回归

### 1.2 更复杂的神经网络
下面是一个四层的神经网络,其中第四层是输出层
![更复杂的神经网络](img/in-post/post-mlnote/image-12.png)

计算使用的符号说明
![符号](img/in-post/post-mlnote/image-13.png)
在这里sigmoid方程也会被称为激活函数(activation function),因为他输出了激活值(activation value).以后我们会看到更多的激活值
输入层$\overrightarrow{x}$也可以记为$a^{[0]}$

### 1.3 推理:做出预测(前向传播forward propagation)
本节我们要讨论手写笔记分析,不过只分析0和1
![笔迹分析](image-14.png)
最后我们使用逻辑回归判别是否为0或1
![alt text](image-15.png)




### 1.4 tensorflow中的数据表示
#### 表示方法一

![tensorflow](img/in-post/post-mlnote/image-16.png)
这一部分用代码表示为
```
x = np.array([[200.0 17.0]])
layer_1 = Dense(units = 3,activation = 'sigmoid')
a1 = layer_1(x)

layer_2 = Dense(units=1 , activation= 'sigmoid')
a2 = layer_2(a1)
```
#### 表示方法二:顺序框架
同样需要布置第一层和第二层,但是可以用顺序框架
```
# 布置框架
layer_1 = Dense(units = 3,activation = 'sigmoid')
layer_2 = Dense(units=1 , activation= 'sigmoid')
model = Sequential([layer_1,layer_2])
# 载入数据
x = np.array(........)
y = np.array(........) # 目标值
#
model.complie(...)
model.fit(x,y)
# 做出预测
model.predict(x_new)

```

一般矩阵用大写字母表示,向量和标量用小写字母表示

### 1.5 tensorflow代码实例
#### 1.5.1 案例一
1. 导入tensorflow库
```
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
```
2. 定义一个具有单个神经元的**简单神经网络模型**
```
model = Sequential(
    [
        tf.keras.layers.Dense(1, input_dim=1,  activation = 'sigmoid', name='L1')
    ]
)
```
- Sequential 是 Keras 中的一种模型类型，它允许你按顺序堆叠各个层来构建神经网络
- Dense 层是一个全连接层，意味着该层中的每个神经元都与前一层的所有神经元相连。
- 1：表示该层中神经元的数量，这里只有一个神经元。
- input_dim=1：指定输入数据的维度，这里表示输入数据是一维的。
- activation = 'sigmoid'：指定该层使用的激活函数为 Sigmoid 函数。Sigmoid 函数的输出范围在 0 到 1 之间，常用于二分类问题。
- name='L1'：为该层指定一个名称，方便后续的模型分析和调试。
3. 检查模型的定义
```
model.summary() # 显示了模型中的层和参数数量
```
4. 从已经构建并训练好的模型中提取权重和偏置
```
logistic_layer = model.get_layer('L1') # 从L1层中提取对象
w, b = logistic_layer.get_weights() 
print(w, b)
print(w.shape, b.shape)
```
5. 手动设置权重和偏置
#### 1.5.2 案例二 咖啡烘焙
1. 归一化
```
# 打印温度（X 的第一列）归一化前的最大值和最小值
print(f"Temperature Max, Min pre normalization: {np.max(X[:,0]):0.2f}, {np.min(X[:,0]):0.2f}")
# 打印持续时间（X 的第二列）归一化前的最大值和最小值
print(f"Duration    Max, Min pre normalization: {np.max(X[:,1]):0.2f}, {np.min(X[:,1]):0.2f}")
# 创建一个归一化层，axis=-1 表示沿着 最后一个轴进行归一化
norm_l = tf.keras.layers.Normalization(axis=-1)
# 让归一化层适应输入数据 X，计算数据的均值和方差
norm_l.adapt(X)  # learns mean, variance
# 使用归一化层对输入数据 X 进行归一化处理，得到归一化后的数据 Xn
Xn = norm_l(X)
# 打印温度（Xn 的第一列）归一化后的最大值和最小值
print(f"Temperature Max, Min post normalization: {np.max(Xn[:,0]):0.2f}, {np.min(Xn[:,0]):0.2f}")
# 打印持续时间（Xn 的第二列）归一化后的最大值和最小值
print(f"Duration    Max, Min post normalization: {np.max(Xn[:,1]):0.2f}, {np.min(Xn[:,1]):0.2f}")
```
2. 扩大数据集
```
# 使用 np.tile 函数将 Xn 沿着行方向重复 1000 次，列方向不重复；将 Y 同样沿着行方向重复 1000 次，列方向不重复
Xt = np.tile(Xn,(1000,1))
Yt= np.tile(Y,(1000,1))   
print(Xt.shape, Yt.shape)   
```
3. 构建神经网络模型
```
# 设置 TensorFlow 的随机种子以保证结果的可重复性
# 在 TensorFlow 中，很多操作（如初始化权重、随机打乱数据等）涉及到随机数的生成。
# 通过设置固定的随机种子 1234，可以确保每次运行代码时这些随机操作的结果都是一致的，方便调试和复现实验结果
tf.random.set_seed(1234)  # applied to achieve consistent results
# 构建神经网络模型
model = Sequential(
    [
        # 定义了模型的输入层，shape=(2,) 表示输入数据的形状为每个样本有 2 个特征。摄氏温度和烘焙时间
        tf.keras.Input(shape=(2,)),
        Dense(3, activation='sigmoid', name = 'layer1'), #3 个神经元
        Dense(1, activation='sigmoid', name = 'layer2')
     ]
)
```
4. 执行梯度下降
- model.compile 语句定义了一个损失函数并指定了一种编译优化。
- model.fit 语句运行梯度下降并拟合权重到数据
```
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),
)
model.fit(
    Xt,Yt,            
    epochs=10,
)
```
5. 更新权重
```
W1, b1 = model.get_layer("layer1").get_weights()
W2, b2 = model.get_layer("layer2").get_weights()
print("W1:\n", W1, "\nb1:", b1)
print("W2:\n", W2, "\nb2:", b2)
```

#### 1.5.3 案例三:==带有 Dropout 层==以防止过拟合的模型
```
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

model3 = Sequential(
    [
        tf.keras.Input(shape=(8,)),  # 输入形状为 8 个特征
        Dense(16, activation='relu', name='layer1'),  # 第一个隐藏层，使用 ReLU 激活函数
        Dropout(0.2, name='dropout1'),  # Dropout 层，随机丢弃 20% 的神经元以防止过拟合
        Dense(8, activation='relu', name='layer2'),  # 第二个隐藏层，使用 ReLU 激活函数
        Dropout(0.2, name='dropout2'),  # Dropout 层，随机丢弃 20% 的神经元
        Dense(1, activation='sigmoid', name='output_layer')  # 输出层，用于二分类任务，使用 Sigmoid 激活函数
    ]
)

```




### 1.6 矩阵乘法
若
$A=\begin{bmatrix}a_{11}&a_{12}&a_{13}\\a_{21}&a_{22}&a_{23}\end{bmatrix}，B=\begin{bmatrix}b_{11}&b_{12}\\b_{21}&b_{22}\\b_{31}&b_{32}\end{bmatrix}$
则矩阵$C=AB$,即$c_{11} = a_{11}b_{11}+a_{12}b_{21}+a_{13}b_{31}$,$c_{12} = a_{11}b_{12}+a_{22}b_{21}+a_{13}b_{32}$,.....
运算性质
- **不满足交换律**:$AB \neq BA$
    例如,$A=\begin{bmatrix}1&1\\0&0\end{bmatrix}，B=\begin{bmatrix}1&0\\1&0\end{bmatrix}，AB=\begin{bmatrix}2&0\\0&0\end{bmatrix}，BA=\begin{bmatrix}1&1\\1&1\end{bmatrix}$
- 满足结合律:$(AB)C = A(BC)$
- 满足分配率:$A(B + C)=AB + AC，(B + C)A=BA + CA$



## 2 人工智能系统











