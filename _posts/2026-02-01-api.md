---
layout:     post
title:      "大模型微调后暴露API方案比较"
date:       2026-02-01 12:00:00
author:     "Summer"
header-img: "img/head/image6.jpg"
catalog: true
mathjax: true
tags:
    - LLM
    - AI For Energy
---

我们来假设以下情形：我们已经在本地部署了官方的模型、或者自己微调部署了一版模型，想要使用这个模型来搭建一个交互平台。比较好的方法是使用工具部署模型并且暴露HTTP接口。

除了在[之前的帖子](https://summerzhouzihan.github.io/2025/07/30/sft/)中提到的FastAPI、vLLM、Ollama，还有Text Generation WebUI（带API模式）、Transformers原生API、TGI（Text Generation Inference）、TensorRT-LLM等主流方案，覆盖从快速测试、中小规模部署到高并发生产级服务的全场景。我们不妨来比较一下这些工具的区别，看看哪些方案比较适合现在的开发需求。

## 一、暴露API方案

实际上，我们之前了解的三个工具恰好代表了三个方向：基础框架开发（FastAPI），开发者和个人工具（Ollama），生产级推理引擎（vLLM）。

[**FastAPI**](https://fastapi.tiangolo.com/) 的定位是**手动挡**。你自己写 Python 代码调用 Hugging Face 代码。能够实现 100% 自定义，可以在接口里塞入复杂的业务逻辑，比如特殊的 Prompt 预处理、多步搜索等等。但是性能极差，没有 PagedAttention 等优化，显存容易溢出，吞吐量可能只有 vLLM 的几分之一。适合科研实验、或者业务逻辑极其复杂且并发极低的场景。

<img src="https://zzh123-1325455460.cos.ap-nanjing.myqcloud.com/logo-teal.png" width="400"/>

**Ollama** 的定位是 LLM 界的 "Docker"。极致简单。一个命令完成下载和运行；自动管理显存和内存切换；提供 OpenAI 兼容接口。但是为了易用性牺牲了部分吞吐性能，不适合承载超高并发的生产业务。适用于**本地开发、个人助手、低频率内部工具**。

<img src="https://zzh123-1325455460.cos.ap-nanjing.myqcloud.com/202602011049033.png" width="400"/>

**vLLM** 是目前**最流行的开源高性能部署引擎**。首创了 [PagedAttention](https://blog.vllm.ai/2023/06/20/vllm.html) 技术，极大地提高了显存利用率；支持连续批处理，能并行处理大量请求。不过对复杂逻辑的深度定制略显繁琐；由于基于 Python（FastAPI）封装，在极端高频小包请求下，网络层性能略逊于 C++ 原生实现。

<img src="https://zzh123-1325455460.cos.ap-nanjing.myqcloud.com/annimation0.gif"/>

下面是其他的工具介绍。

### 1. Text Generation WebUI（TGUI）

[TGUI](https://github.com/oobabooga/text-generation-webui) 是基于Gradio开发的大模型可视化交互工具，**内置完整API接口**，兼容OpenAI格式，无需额外开发代码，启动时开启`--api`参数即可暴露RESTful API，支持绝大多数开源大模型（LLaMA、Qwen、Mistral等），支持量化（4/8bit）、多卡加载、参数微调后的模型直接加载。优势是零代码、可视化调试+API服务一体、支持多模型格式/量化、生态丰富（插件多）；但是高并发下性能一般，适合测试/小流量场景，非纯生产级服务框架。

### 2. Transformers原生API

Hugging Face Transformers库**内置的简单API服务能力**，基于FastAPI封装，直接调用`pipeline`或`AutoModel`加载微调后的模型，一行命令即可启动API服务，完全兼容Hugging Face生态的模型格式（微调后的模型可直接加载）。核心优势是原生适配HF微调模型、轻量无额外依赖、代码极简、易二次开发；但是无性能优化（无动态批处理、量化支持弱），仅适合小模型/本地测试，不支持高并发。

### 3. TGI（Text Generation Inference）

Hugging Face官方推出的**生产级大模型推理/API服务框架**，专为大模型优化，深度适配HF生态，支持微调后的模型直接部署，内置动态批处理、张量并行、量化（GPTQ/AWQ/4bit）、流式输出，且**兼容OpenAI API格式**，可直接对接现有基于OpenAI的应用。优势是生产级性能、HF生态原生适配、高并发支持、标准化API、多卡/分布式部署。不足是部署稍复杂（推荐Docker）、对硬件资源有一定要求，轻量测试场景稍显冗余。

### 4. TensorRT-LLM 

[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) 是英伟达推出的**大模型高性能推理引擎**，基于TensorRT做极致优化（算子融合、量化、张量并行/流水线并行），支持将微调后的大模型转换为TensorRT引擎格式，再通过其内置的API服务模块暴露接口，**是目前GPU推理性能天花板**。拥有极致推理速度，比vLLM更快，尤其英伟达A100/H100等高端卡，支持大模型（70B/130B）高效部署，多卡多节点优化；但是现在仅支持英伟达GPU、部署复杂（需模型转换）、对技术要求高，适合大规模生产级部署。

<img src="https://zzh123-1325455460.cos.ap-nanjing.myqcloud.com/202602011051990.png" width="400"/>

> 需要提醒你一下，模型变大并不仅仅是参数变大那么简单，会多出很多麻烦的事情！

关于TensorRT-LLM与vllm等方案的比较，参考 [YouTube-AI Serving Frameworks Explained: vLLM vs TensorRT-LLM vs Ray Serve](https://www.youtube.com/watch?v=p6GspvZQ5WU)

## 二、各方案比较

### 1. 性能

- **vLLM/TensorRT-LLM**：采用专属高性能推理架构（vLLM的PagedAttention、TensorRT-LLM的TensorRT算子优化），解决大模型推理的内存碎片和批处理效率问题，**是高并发的核心选择**；
- **TGI**：基于Hugging Face `accelerate`和`bitsandbytes`开发，适配HF生态的同时做了生产级优化，平衡性能和兼容性；
- **Ollama/TGUI/Transformers**：基于基础推理逻辑开发，无专属高性能架构，仅做基础量化和资源调度，适合非高并发场景；
- **FastAPI/Transformers原生**：无底层优化，仅做接口封装，性能完全依赖模型本身和硬件。

### 2. 开发与部署复杂度

- **零代码/一键部署**：Ollama（`ollama serve`）、TGUI（`python server.py --api`）、Transformers原生（一行命令启动），无需编写接口代码，适合快速上手；
- **低代码部署**：vLLM（`python -m vllm.serve.openai_api_server --model 你的微调模型`）、TGI（Docker一键启动），仅需指定模型路径即可，支持多参数配置；
- **需开发定制**：FastAPI原生（需手动编写模型加载、接口逻辑、请求处理）、TensorRT-LLM（需手动转换模型为TensorRT引擎，编写服务代码），适合个性化需求。

### 3. 模型兼容性差异

- **全兼容**：FastAPI、Transformers原生、TGUI，支持所有开源模型格式（HF、GGUF、GPTQ等），微调后的模型无需转换；
- **HF生态优先**：TGI、Transformers原生，对HF格式（`.bin/.safetensors`）微调模型原生支持，其他格式需转换；
- **GGUF格式优先**：Ollama，主打GGUF量化格式（微调模型需转换为GGUF），加载速度快、占用内存小；
- **英伟达生态**：TensorRT-LLM，仅支持英伟达GPU，模型需转换为TensorRT引擎格式，兼容性稍弱但性能极致；
- **主流兼容**：vLLM，支持HF、GPTQ、AWQ、GGUF等绝大多数格式，微调后的HF模型可直接加载，平衡兼容性和性能。

### 4. API标准化差异

- **兼容OpenAI格式**：vLLM、TGI、TGUI（开启插件）、Ollama，可直接使用`/v1/completions`、`/v1/chat/completions`接口，对接现有应用（如ChatGPT客户端、LangChain），**无需修改应用代码**；
- **自定义接口**：FastAPI原生、Transformers原生，需手动定义接口路径和参数，灵活性高但无标准化。

## 三、总结

如果要上线一个正式产品，可以选 vLLM。它平衡了性能与易用性，且社区支持最广。

如果只是想在本地写个程序调模型，可以选 Ollama。

如果在顶级大厂搞大模型基座，需要请组建专门团队折腾 TensorRT-LLM。

如果需要处理**非常规**的业务逻辑，可以考虑用 FastAPI 封装 vLLM 的核心引擎，而不是封装 Transformers 原生代码。

所有方案中，**vLLM**是目前**性价比最高**的选择。低开发成本、极致性能、高兼容性，覆盖绝大多数微调模型的API暴露场景。

